"""Pytest configuration and fixtures for performance benchmarks.

Benchmarks record wall time, token estimates, and phase breakdown. With real LLM
(AI_TEAM_BENCHMARK_FULL=1 or AI_TEAM_USE_REAL_LLM=1), crews run against Ollama;
otherwise tests use mocks and record structure only. Results are written to
docs/benchmark_results.json and docs/performance_report.md at session end.
"""

from __future__ import annotations

import os
from pathlib import Path
from typing import Any, Dict, List

import pytest


def pytest_configure(config: Any) -> None:
    """Register performance markers."""
    config.addinivalue_line(
        "markers",
        "performance: performance benchmark tests; record timings and write benchmark_results.json.",
    )
    config.addinivalue_line(
        "markers",
        "benchmark_real: benchmark with real Ollama when AI_TEAM_BENCHMARK_FULL=1.",
    )


@pytest.fixture(scope="session")
def run_real_benchmarks() -> bool:
    """Whether to run real crew/flow benchmarks (requires Ollama)."""
    return os.environ.get("AI_TEAM_BENCHMARK_FULL", "").lower() in ("1", "true", "yes") or os.environ.get(
        "AI_TEAM_USE_REAL_LLM", ""
    ).lower() in ("1", "true", "yes")


@pytest.fixture(scope="session")
def benchmark_results_dir() -> Path:
    """Directory for benchmark artifacts (docs/)."""
    return Path(__file__).resolve().parent.parent.parent / "docs"


@pytest.fixture(scope="session")
def benchmark_collector() -> Dict[str, Any]:
    """Session-scoped dict to collect benchmark results; written at session teardown."""
    collector: Dict[str, Any] = {
        "crews": {},
        "full_flow": {},
        "phase_budgets": {
            "requirements_min": 60,
            "architecture_min": 120,
            "development_min": 240,
            "qa_min": 120,
            "deployment_min": 60,
        },
        "phase_times_seconds": {},
        "bottlenecks": [],
        "hardware_profiles": {},
        "token_estimation": {},
        "meta": {"run_real": False},
    }
    return collector


def _write_benchmark_artifacts(
    collector: Dict[str, Any],
    results_dir: Path,
) -> None:
    """Write docs/benchmark_results.json and docs/performance_report.md."""
    import json

    results_dir.mkdir(parents=True, exist_ok=True)
    json_path = results_dir / "benchmark_results.json"
    report_path = results_dir / "performance_report.md"

    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(collector, f, indent=2)

    lines: List[str] = [
        "# Performance Report",
        "",
        "Generated by `pytest tests/performance/ -v -s --tb=short`.",
        "",
        "## Run metadata",
        "",
        f"- Real LLM: {collector.get('meta', {}).get('run_real', False)}",
        "",
        "## Per-crew benchmarks",
        "",
    ]
    for name, data in collector.get("crews", {}).items():
        lines.append(f"### {name}")
        lines.append("")
        for k, v in data.items():
            lines.append(f"- **{k}**: {v}")
        lines.append("")

    lines.append("## Full flow (Demo 1)")
    lines.append("")
    for k, v in collector.get("full_flow", {}).items():
        lines.append(f"- **{k}**: {v}")
    lines.append("")

    if collector.get("phase_times_seconds"):
        lines.append("## Phase times (seconds)")
        lines.append("")
        for phase, sec in collector["phase_times_seconds"].items():
            lines.append(f"- **{phase}**: {sec:.2f}s")
        lines.append("")

    if collector.get("bottlenecks"):
        lines.append("## Bottlenecks")
        lines.append("")
        for b in collector["bottlenecks"]:
            lines.append(f"- {b}")
        lines.append("")

    if collector.get("hardware_profiles"):
        lines.append("## Hardware profiles")
        lines.append("")
        for profile, info in collector["hardware_profiles"].items():
            lines.append(f"### {profile}")
            for k, v in info.items():
                lines.append(f"- **{k}**: {v}")
            lines.append("")

    if collector.get("token_estimation"):
        lines.append("## Token estimation")
        lines.append("")
        for k, v in collector["token_estimation"].items():
            lines.append(f"- **{k}**: {v}")
        lines.append("")
    lines.append("---")
    lines.append("")
    report_path.write_text("\n".join(lines), encoding="utf-8")


@pytest.fixture(scope="session", autouse=True)
def _save_benchmark_results(
    benchmark_collector: Dict[str, Any],
    benchmark_results_dir: Path,
) -> None:
    """After all performance tests, write benchmark_results.json and performance_report.md."""
    yield
    _write_benchmark_artifacts(benchmark_collector, benchmark_results_dir)
