# ai-team environment template
# Copy to .env and adjust values.

# Ollama
OLLAMA_BASE_URL=http://localhost:11434
# OLLAMA_MEMORY_PRESET=default  # or 32gb (7B/8B per-role), 32gb_single (one model for all)
# OLLAMA_SINGLE_MODEL=qwen2.5-coder:7b  # When set, all agents use this model (recommended for 32GB RAM)
# AI_TEAM_USE_REAL_LLM=0  # Set to 1 to run integration tests with real Ollama

# Per-role models (optional; ignored when OLLAMA_SINGLE_MODEL or 32gb_single is set)
# OLLAMA_MANAGER_MODEL=
# OLLAMA_PRODUCT_OWNER_MODEL=
# OLLAMA_ARCHITECT_MODEL=
# OLLAMA_BACKEND_DEV_MODEL=
# OLLAMA_FRONTEND_DEV_MODEL=
# OLLAMA_DEVOPS_MODEL=
# OLLAMA_CLOUD_MODEL=
# OLLAMA_QA_MODEL=

# Project / crew
# PROJECT_PLANNING_SEQUENTIAL=1  # Use sequential planning for Ollama: avoids "Instructor multiple tool calls" and "Failed to add to long term memory"

# Guardrails
# GUARDRAIL_MAX_RETRIES=3
# CODE_QUALITY_MIN_SCORE=0.7
# TEST_COVERAGE_MIN=0.6
# MAX_FILE_SIZE_KB=500

# UI (Gradio)
# GRADIO_SERVER_PORT=7860
