# ai-team environment template
# Copy to .env and adjust values.

# =============================================================================
# OPENROUTER (inference â€” 3-tier: dev / test / prod)
# =============================================================================
# API key from https://openrouter.ai/settings/keys
OPENROUTER_API_KEY=sk-or-v1-your-key-here
# Active tier: dev (cheapest), test (better quality), prod (portfolio demos)
AI_TEAM_ENV=dev
# OpenRouter endpoint; leave as-is unless using a proxy
OPENROUTER_API_BASE=https://openrouter.ai/api/v1
# Shown on OpenRouter leaderboards
OR_SITE_URL=https://github.com/yourusername/ai-team
OR_APP_NAME=AI-Team-Capstone

# Cost controls
# Max USD per pipeline run; run aborts if estimate exceeds this
AI_TEAM_MAX_COST_PER_RUN=5.00
# Show cost estimate before execution (true/false)
AI_TEAM_SHOW_COST_ESTIMATE=true
# Require confirmation before prod runs
AI_TEAM_PROD_CONFIRM=true

# =============================================================================
# OLLAMA (embeddings only when using OpenRouter; full inference if not)
# =============================================================================
OLLAMA_BASE_URL=http://localhost:11434
# When using OpenRouter, embeddings stay local (e.g. nomic-embed-text)
OLLAMA_EMBEDDING_MODEL=nomic-embed-text
# OLLAMA_MEMORY_PRESET=default  # or 32gb (7B/8B per-role), 32gb_single (one model for all)
# OLLAMA_SINGLE_MODEL=qwen2.5-coder:7b  # When set, all agents use this model (recommended for 32GB RAM)
# AI_TEAM_USE_REAL_LLM=0  # Set to 1 to run integration tests with real Ollama

# Per-role models (optional; ignored when OLLAMA_SINGLE_MODEL or 32gb_single is set)
# OLLAMA_MANAGER_MODEL=
# OLLAMA_PRODUCT_OWNER_MODEL=
# OLLAMA_ARCHITECT_MODEL=
# OLLAMA_BACKEND_DEV_MODEL=
# OLLAMA_FRONTEND_DEV_MODEL=
# OLLAMA_DEVOPS_MODEL=
# OLLAMA_CLOUD_MODEL=
# OLLAMA_QA_MODEL=

# Project / crew
# PROJECT_PLANNING_SEQUENTIAL=1  # Use sequential planning for Ollama: avoids "Instructor multiple tool calls" and "Failed to add to long term memory"

# =============================================================================
# GUARDRAILS
# =============================================================================
# GUARDRAIL_MAX_RETRIES=3
# GUARDRAIL_RETRY_DELAY=1.0
# CODE_QUALITY_MIN_SCORE=0.7
# TEST_COVERAGE_MIN=0.6
# MAX_FILE_SIZE_KB=500

# =============================================================================
# MEMORY
# =============================================================================
# MEMORY_PROVIDER=chromadb
# CHROMADB_PERSIST_DIR=./data/chromadb

# UI (Gradio)
# GRADIO_SERVER_PORT=7860
